{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Multilingual Named Entity Recognition\n\n- Dataset: Subset of Cross-lingual TRansfer Evaluation of Multilingual Encoders (XTREME) benchmark, PAN-X, which is a dataset of Wikipedia articles in different languages in IOB2 format ","metadata":{}},{"cell_type":"code","source":"from datasets import get_dataset_config_names\nimport pandas as pd\nfrom IPython.core.display import display\nimport numpy as np\n\nfull_context = pd.option_context('display.max_rows', 10, 'display.max_columns', 500)","metadata":{"execution":{"iopub.status.busy":"2022-12-31T14:48:48.045735Z","iopub.execute_input":"2022-12-31T14:48:48.046322Z","iopub.status.idle":"2022-12-31T14:48:49.109078Z","shell.execute_reply.started":"2022-12-31T14:48:48.046204Z","shell.execute_reply":"2022-12-31T14:48:49.107752Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"xtreme_subsets = get_dataset_config_names(\"xtreme\")\nlen(xtreme_subsets)","metadata":{"execution":{"iopub.status.busy":"2022-12-31T14:48:49.111500Z","iopub.execute_input":"2022-12-31T14:48:49.112341Z","iopub.status.idle":"2022-12-31T14:48:52.098717Z","shell.execute_reply.started":"2022-12-31T14:48:49.112292Z","shell.execute_reply":"2022-12-31T14:48:52.097409Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/9.09k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf17a1bcdbda4e9ca0170e76645d5ee4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/23.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66c09722de1a48b69fbb04e9af4a5950"}},"metadata":{}},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"183"},"metadata":{}}]},{"cell_type":"markdown","source":"This dataset has a lot of different configurations. Let's look at the PAN-X datasets:","metadata":{}},{"cell_type":"code","source":"[s for s in xtreme_subsets if s.startswith(\"PAN\")][:10]","metadata":{"execution":{"iopub.status.busy":"2022-12-31T14:48:52.100165Z","iopub.execute_input":"2022-12-31T14:48:52.101133Z","iopub.status.idle":"2022-12-31T14:48:52.110224Z","shell.execute_reply.started":"2022-12-31T14:48:52.101085Z","shell.execute_reply":"2022-12-31T14:48:52.109040Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"['PAN-X.af',\n 'PAN-X.ar',\n 'PAN-X.bg',\n 'PAN-X.bn',\n 'PAN-X.de',\n 'PAN-X.el',\n 'PAN-X.en',\n 'PAN-X.es',\n 'PAN-X.et',\n 'PAN-X.eu']"},"metadata":{}}]},{"cell_type":"code","source":"from datasets import load_dataset, DatasetDict\nfrom collections import defaultdict","metadata":{"execution":{"iopub.status.busy":"2022-12-31T14:48:52.113449Z","iopub.execute_input":"2022-12-31T14:48:52.114102Z","iopub.status.idle":"2022-12-31T14:48:52.125118Z","shell.execute_reply.started":"2022-12-31T14:48:52.114064Z","shell.execute_reply":"2022-12-31T14:48:52.124078Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"langs = [\"de\", \"fr\", \"it\", \"en\"]\nfracs = [0.629, 0.229, 0.084, 0.059]\n\npanx = defaultdict(DatasetDict)\n\nfor lang, frac in zip(langs, fracs):\n    # load monolingual corpus\n    ds = load_dataset(\"xtreme\", name=f\"PAN-X.{lang}\")\n    # shuffle and downsample to spoken proportion\n    for split in ds:\n        panx[lang][split] = (ds[split]\n                             .shuffle(seed=0)\n                            .select(range(int(frac * ds[split].num_rows))))\n","metadata":{"execution":{"iopub.status.busy":"2022-12-31T14:48:52.126801Z","iopub.execute_input":"2022-12-31T14:48:52.127569Z","iopub.status.idle":"2022-12-31T14:49:58.742376Z","shell.execute_reply.started":"2022-12-31T14:48:52.127526Z","shell.execute_reply":"2022-12-31T14:49:58.740990Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Downloading and preparing dataset xtreme/PAN-X.de (download: 223.17 MiB, generated: 9.08 MiB, post-processed: Unknown size, total: 232.25 MiB) to /root/.cache/huggingface/datasets/xtreme/PAN-X.de/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/234M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0652225f0dd649a3adec10262a71c935"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/20000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset xtreme downloaded and prepared to /root/.cache/huggingface/datasets/xtreme/PAN-X.de/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d61b1124b2014edc8ad67a8eb91af239"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset xtreme/PAN-X.fr (download: 223.17 MiB, generated: 6.37 MiB, post-processed: Unknown size, total: 229.53 MiB) to /root/.cache/huggingface/datasets/xtreme/PAN-X.fr/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/20000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset xtreme downloaded and prepared to /root/.cache/huggingface/datasets/xtreme/PAN-X.fr/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d54bbbdebb1f4209885c49bdc6f41d5a"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset xtreme/PAN-X.it (download: 223.17 MiB, generated: 7.35 MiB, post-processed: Unknown size, total: 230.52 MiB) to /root/.cache/huggingface/datasets/xtreme/PAN-X.it/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/20000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset xtreme downloaded and prepared to /root/.cache/huggingface/datasets/xtreme/PAN-X.it/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d364bc5a09ef417ab36f150b572967e1"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset xtreme/PAN-X.en (download: 223.17 MiB, generated: 7.30 MiB, post-processed: Unknown size, total: 230.47 MiB) to /root/.cache/huggingface/datasets/xtreme/PAN-X.en/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/20000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset xtreme downloaded and prepared to /root/.cache/huggingface/datasets/xtreme/PAN-X.en/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b494610005a43958ce48a8ab1bdfed3"}},"metadata":{}}]},{"cell_type":"code","source":"counts = pd.DataFrame({lang: [panx[lang]['train'].num_rows] for lang in langs}, index=[\"Number of training examples\"])\ncounts/counts.sum(axis=1)[0]","metadata":{"execution":{"iopub.status.busy":"2022-12-31T14:49:58.743595Z","iopub.execute_input":"2022-12-31T14:49:58.744410Z","iopub.status.idle":"2022-12-31T14:49:58.770158Z","shell.execute_reply.started":"2022-12-31T14:49:58.744365Z","shell.execute_reply":"2022-12-31T14:49:58.768882Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                                   de        fr        it        en\nNumber of training examples  0.628372  0.228771  0.083916  0.058941","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>de</th>\n      <th>fr</th>\n      <th>it</th>\n      <th>en</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Number of training examples</th>\n      <td>0.628372</td>\n      <td>0.228771</td>\n      <td>0.083916</td>\n      <td>0.058941</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"panx[\"de\"][\"train\"][0]","metadata":{"execution":{"iopub.status.busy":"2022-12-31T14:49:58.771964Z","iopub.execute_input":"2022-12-31T14:49:58.772523Z","iopub.status.idle":"2022-12-31T14:49:58.783661Z","shell.execute_reply.started":"2022-12-31T14:49:58.772472Z","shell.execute_reply":"2022-12-31T14:49:58.782224Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"{'tokens': ['2.000',\n  'Einwohnern',\n  'an',\n  'der',\n  'Danziger',\n  'Bucht',\n  'in',\n  'der',\n  'polnischen',\n  'Woiwodschaft',\n  'Pommern',\n  '.'],\n 'ner_tags': [0, 0, 0, 0, 5, 6, 0, 0, 5, 5, 6, 0],\n 'langs': ['de',\n  'de',\n  'de',\n  'de',\n  'de',\n  'de',\n  'de',\n  'de',\n  'de',\n  'de',\n  'de',\n  'de']}"},"metadata":{}}]},{"cell_type":"code","source":"panx[\"de\"][\"train\"].features","metadata":{"execution":{"iopub.status.busy":"2022-12-31T14:49:58.785522Z","iopub.execute_input":"2022-12-31T14:49:58.786028Z","iopub.status.idle":"2022-12-31T14:49:58.798218Z","shell.execute_reply.started":"2022-12-31T14:49:58.785986Z","shell.execute_reply":"2022-12-31T14:49:58.796874Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"{'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n 'ner_tags': Sequence(feature=ClassLabel(num_classes=7, names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'], id=None), length=-1, id=None),\n 'langs': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}"},"metadata":{}}]},{"cell_type":"markdown","source":"So the `ner_tags` are [IOB2 tags](https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)).","metadata":{}},{"cell_type":"code","source":"tags = panx[\"de\"][\"train\"].features[\"ner_tags\"].feature\ntags","metadata":{"execution":{"iopub.status.busy":"2022-12-31T14:49:58.800128Z","iopub.execute_input":"2022-12-31T14:49:58.800515Z","iopub.status.idle":"2022-12-31T14:49:58.812059Z","shell.execute_reply.started":"2022-12-31T14:49:58.800481Z","shell.execute_reply":"2022-12-31T14:49:58.810753Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"ClassLabel(num_classes=7, names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'], id=None)"},"metadata":{}}]},{"cell_type":"code","source":"from typing import Dict, List \n\ndef create_tag_names(batch: Dict[str, List]) -> Dict[str, List]:\n    return {'ner_tags_str': [tags.int2str(idx) for idx in batch[\"ner_tags\"]]}","metadata":{"execution":{"iopub.status.busy":"2022-12-31T14:49:58.816075Z","iopub.execute_input":"2022-12-31T14:49:58.817505Z","iopub.status.idle":"2022-12-31T14:49:58.825908Z","shell.execute_reply.started":"2022-12-31T14:49:58.817444Z","shell.execute_reply":"2022-12-31T14:49:58.824743Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## Creating a custom model for token classification\n\nNER can be viewed as token classification. Let's generate a custom model head for this purpose.","metadata":{}},{"cell_type":"code","source":"from typing import Optional\n\nimport torch\nimport torch.nn as nn\nfrom transformers import XLMRobertaConfig\nfrom transformers.configuration_utils import PretrainedConfig\nfrom transformers.modeling_outputs import TokenClassifierOutput\nfrom transformers.models.roberta.modeling_roberta import RobertaModel, RobertaPreTrainedModel\n\n\nclass XLMRobertaForTokenClassification(RobertaPreTrainedModel):  # inherits from model body. Knows how to load pretrained weights.\n    config_class = XLMRobertaConfig  # so that we use XLM-R config instead of Roberta\n\n    def __init__(self, config: PretrainedConfig):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n        # Load model body\n        self.roberta = RobertaModel(config, add_pooling_layer=False)  # return all hidden states, not just the [CLS] ones\n        # Set up token classification head\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n        # Load and initialize weights\n        self.init_weights()\n\n    def forward(self, input_ids: Optional[torch.Tensor] = None, attention_mask: Optional[torch.Tensor] = None,\n                token_type_ids: Optional[torch.Tensor] = None, labels: Optional[torch.Tensor] = None, **kwargs):\n        # Use model body to get encoder representations\n        outputs = self.roberta(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, **kwargs)\n        # Apply classifier to encoder representations\n        sequence_output = self.dropout(outputs[0])\n        logits = self.classifier(sequence_output)\n        # Calculate losses\n        loss = None\n        if labels is not None:            \n            loss_fct = nn.CrossEntropyLoss()\n            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n        # Return model output object\n        return TokenClassifierOutput(loss=loss, logits=logits, hidden_states=outputs.hidden_states,\n                                     attentions=outputs.attentions)\n","metadata":{"execution":{"iopub.status.busy":"2022-12-31T14:49:58.827852Z","iopub.execute_input":"2022-12-31T14:49:58.828766Z","iopub.status.idle":"2022-12-31T14:50:02.750950Z","shell.execute_reply.started":"2022-12-31T14:49:58.828674Z","shell.execute_reply":"2022-12-31T14:50:02.749128Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"XLMRobertaForTokenClassification.__mro__","metadata":{"execution":{"iopub.status.busy":"2022-12-31T14:50:02.753214Z","iopub.execute_input":"2022-12-31T14:50:02.753634Z","iopub.status.idle":"2022-12-31T14:50:02.762971Z","shell.execute_reply.started":"2022-12-31T14:50:02.753595Z","shell.execute_reply":"2022-12-31T14:50:02.761684Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"(__main__.XLMRobertaForTokenClassification,\n transformers.models.roberta.modeling_roberta.RobertaPreTrainedModel,\n transformers.modeling_utils.PreTrainedModel,\n torch.nn.modules.module.Module,\n transformers.modeling_utils.ModuleUtilsMixin,\n transformers.generation_utils.GenerationMixin,\n transformers.utils.hub.PushToHubMixin,\n object)"},"metadata":{}}]},{"cell_type":"markdown","source":"## Loading a custom model","metadata":{}},{"cell_type":"code","source":"tags.names","metadata":{"execution":{"iopub.status.busy":"2022-12-31T14:50:02.764592Z","iopub.execute_input":"2022-12-31T14:50:02.766259Z","iopub.status.idle":"2022-12-31T14:50:02.783742Z","shell.execute_reply.started":"2022-12-31T14:50:02.766206Z","shell.execute_reply":"2022-12-31T14:50:02.782185Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']"},"metadata":{}}]},{"cell_type":"code","source":"index2tag = {idx: tag for idx, tag in enumerate(tags.names)}\ntag2index = {tag: idx for idx, tag in enumerate(tags.names)}","metadata":{"execution":{"iopub.status.busy":"2022-12-31T14:50:02.785466Z","iopub.execute_input":"2022-12-31T14:50:02.786519Z","iopub.status.idle":"2022-12-31T14:50:02.795675Z","shell.execute_reply.started":"2022-12-31T14:50:02.786464Z","shell.execute_reply":"2022-12-31T14:50:02.794408Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoConfig, AutoTokenizer","metadata":{"execution":{"iopub.status.busy":"2022-12-31T14:50:02.797237Z","iopub.execute_input":"2022-12-31T14:50:02.797700Z","iopub.status.idle":"2022-12-31T14:50:02.818397Z","shell.execute_reply.started":"2022-12-31T14:50:02.797668Z","shell.execute_reply":"2022-12-31T14:50:02.817131Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"xlmr_model_name = \"xlm-roberta-base\"\n\n# Loading the config and overwriting parameters of the original pretrained model we want to change allows us to customize models\nxlmr_config = AutoConfig.from_pretrained(xlmr_model_name, \n                                         num_labels=tags.num_classes, id2label=index2tag, label2id=tag2index)","metadata":{"execution":{"iopub.status.busy":"2022-12-31T14:50:02.820582Z","iopub.execute_input":"2022-12-31T14:50:02.821425Z","iopub.status.idle":"2022-12-31T14:50:04.764310Z","shell.execute_reply.started":"2022-12-31T14:50:02.821375Z","shell.execute_reply":"2022-12-31T14:50:04.762876Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/615 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3825511dbd9d49029454d6d1c80feb1e"}},"metadata":{}}]},{"cell_type":"code","source":"import torch","metadata":{"execution":{"iopub.status.busy":"2022-12-31T14:50:04.766244Z","iopub.execute_input":"2022-12-31T14:50:04.766734Z","iopub.status.idle":"2022-12-31T14:50:04.772842Z","shell.execute_reply.started":"2022-12-31T14:50:04.766669Z","shell.execute_reply":"2022-12-31T14:50:04.771595Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nxlmr_model = (XLMRobertaForTokenClassification\n              .from_pretrained(xlmr_model_name, config=xlmr_config)\n              .to(device))","metadata":{"execution":{"iopub.status.busy":"2022-12-31T14:50:04.774612Z","iopub.execute_input":"2022-12-31T14:50:04.775113Z","iopub.status.idle":"2022-12-31T14:51:42.635101Z","shell.execute_reply.started":"2022-12-31T14:50:04.775068Z","shell.execute_reply":"2022-12-31T14:51:42.633690Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.04G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39369a6670cc45aaa56e2d4e5bce8c12"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'roberta.embeddings.position_ids', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Test the tokenizer works\ntext = \"Jack Sparrow loves New York!\"\nxlmr_tokenizer = AutoTokenizer.from_pretrained(xlmr_model_name)","metadata":{"execution":{"iopub.status.busy":"2022-12-31T14:51:42.638912Z","iopub.execute_input":"2022-12-31T14:51:42.640019Z","iopub.status.idle":"2022-12-31T14:51:57.723603Z","shell.execute_reply.started":"2022-12-31T14:51:42.639964Z","shell.execute_reply":"2022-12-31T14:51:57.722374Z"},"trusted":true},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/4.83M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a368455113e640b1bd467f8e71b8a717"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/8.68M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6ea7a77bad9479da48c0d6b11ef6e2b"}},"metadata":{}}]},{"cell_type":"code","source":"def tag_text(text, tags, model, tokenizer):\n    # Get tokens with special characters\n    tokens = tokenizer(text).tokens()\n    # Encode the sequence into IDs\n    input_ids = xlmr_tokenizer(text, return_tensors=\"pt\").input_ids.to(device)\n    # Get predictions as distribution over 7 possible classes\n    outputs = model(input_ids).logits\n    # Take argmax to get most likely class per token\n    predictions = torch.argmax(outputs, dim=-1)\n    # Convert to DataFrame\n    preds = [tags.names[p] for p in predictions[0].cpu().numpy()]\n    return pd.DataFrame([tokens, preds], index=[\"Tokens\", \"Tags\"])","metadata":{"execution":{"iopub.status.busy":"2022-12-31T14:51:57.725348Z","iopub.execute_input":"2022-12-31T14:51:57.725738Z","iopub.status.idle":"2022-12-31T14:51:57.733414Z","shell.execute_reply.started":"2022-12-31T14:51:57.725693Z","shell.execute_reply":"2022-12-31T14:51:57.732168Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"tag_text(text, tags, xlmr_model, xlmr_tokenizer)","metadata":{"execution":{"iopub.status.busy":"2022-12-31T14:51:57.735292Z","iopub.execute_input":"2022-12-31T14:51:57.736040Z","iopub.status.idle":"2022-12-31T14:51:57.925608Z","shell.execute_reply.started":"2022-12-31T14:51:57.735983Z","shell.execute_reply":"2022-12-31T14:51:57.924302Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"            0      1      2    3      4  5     6      7  8      9\nTokens    <s>  ▁Jack  ▁Spar  row  ▁love  s  ▁New  ▁York  !   </s>\nTags    B-LOC      O      O    O      O  O     O      O  O  B-LOC","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Tokens</th>\n      <td>&lt;s&gt;</td>\n      <td>▁Jack</td>\n      <td>▁Spar</td>\n      <td>row</td>\n      <td>▁love</td>\n      <td>s</td>\n      <td>▁New</td>\n      <td>▁York</td>\n      <td>!</td>\n      <td>&lt;/s&gt;</td>\n    </tr>\n    <tr>\n      <th>Tags</th>\n      <td>B-LOC</td>\n      <td>O</td>\n      <td>O</td>\n      <td>O</td>\n      <td>O</td>\n      <td>O</td>\n      <td>O</td>\n      <td>O</td>\n      <td>O</td>\n      <td>B-LOC</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"ignore_index_value_in_loss = -100\n\ndef tokenize_and_align_labels(examples: Dict[str, List]) -> Dict[str, List]:\n    \"\"\"Create word-level labels to subword tokens\"\"\"\n    \n    # XLM tokenize a sentence into subwords, where the sentence is split into words\n    tokenized_inputs = xlmr_tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n    labels = []\n    for idx, label in enumerate(examples[\"ner_tags\"]):\n        # assign a subword a word-level integer index, and None to special tokens like <s>, </s>\n        word_ids = tokenized_inputs.word_ids(batch_index=idx)  \n        \n        previous_word_idx = None\n        label_ids = []\n        for word_idx in word_ids:\n            if word_idx is None or word_idx == previous_word_idx:\n                # We are following the convention that only the first subword token be associated with a classification\n                # otherwise we will more heavily weight words that are broken up into multiple subtokens. Thus, when\n                # word_idx == previous_word_idx, we will use a special label to tell the loss function to ignore this index\n                # via the ignore_index flag for nn.CrossEntropyLoss\n                label_ids.append(ignore_index_value_in_loss)\n            else:\n                label_ids.append(label[word_idx])\n                previous_word_idx = word_idx\n        labels.append(label_ids)\n    tokenized_inputs[\"labels\"] = labels\n    return tokenized_inputs\n","metadata":{"execution":{"iopub.status.busy":"2022-12-31T14:51:57.926994Z","iopub.execute_input":"2022-12-31T14:51:57.927355Z","iopub.status.idle":"2022-12-31T14:51:57.937363Z","shell.execute_reply.started":"2022-12-31T14:51:57.927324Z","shell.execute_reply":"2022-12-31T14:51:57.936423Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"example = panx[\"de\"][\"train\"][:3]\npd.DataFrame([example['tokens'][0]], index=['tokens'])","metadata":{"execution":{"iopub.status.busy":"2022-12-31T14:51:57.938900Z","iopub.execute_input":"2022-12-31T14:51:57.939889Z","iopub.status.idle":"2022-12-31T14:51:57.973763Z","shell.execute_reply.started":"2022-12-31T14:51:57.939850Z","shell.execute_reply":"2022-12-31T14:51:57.972434Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"           0           1   2    3         4      5   6    7           8   \\\ntokens  2.000  Einwohnern  an  der  Danziger  Bucht  in  der  polnischen   \n\n                  9        10 11  \ntokens  Woiwodschaft  Pommern  .  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>tokens</th>\n      <td>2.000</td>\n      <td>Einwohnern</td>\n      <td>an</td>\n      <td>der</td>\n      <td>Danziger</td>\n      <td>Bucht</td>\n      <td>in</td>\n      <td>der</td>\n      <td>polnischen</td>\n      <td>Woiwodschaft</td>\n      <td>Pommern</td>\n      <td>.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"tokenized_input = xlmr_tokenizer(example['tokens'][0], truncation=True, is_split_into_words=True)\ntokens = xlmr_tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\nexample_labelled = tokenize_and_align_labels(panx[\"de\"][\"train\"][:3])\n\nwith full_context:\n    display(pd.DataFrame([tokens, example_labelled['labels'][0]], index=['tokens', 'labels']))","metadata":{"execution":{"iopub.status.busy":"2022-12-31T14:51:57.975135Z","iopub.execute_input":"2022-12-31T14:51:57.975556Z","iopub.status.idle":"2022-12-31T14:51:58.008269Z","shell.execute_reply.started":"2022-12-31T14:51:57.975515Z","shell.execute_reply":"2022-12-31T14:51:58.006860Z"},"trusted":true},"execution_count":24,"outputs":[{"output_type":"display_data","data":{"text/plain":"          0       1           2     3    4     5     6     7     8      9   \\\ntokens   <s>  ▁2.000  ▁Einwohner     n  ▁an  ▁der  ▁Dan    zi   ger  ▁Buch   \nlabels  -100       0           0  -100    0     0     5  -100  -100      6   \n\n          10   11    12      13     14   15    16    17      18   19    20  \\\ntokens     t  ▁in  ▁der  ▁polni  schen  ▁Wo     i   wod  schaft  ▁Po  mmer   \nlabels  -100    0     0       5   -100    5  -100  -100    -100    6  -100   \n\n          21 22    23    24  \ntokens     n  ▁     .  </s>  \nlabels  -100  0  -100  -100  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n      <th>12</th>\n      <th>13</th>\n      <th>14</th>\n      <th>15</th>\n      <th>16</th>\n      <th>17</th>\n      <th>18</th>\n      <th>19</th>\n      <th>20</th>\n      <th>21</th>\n      <th>22</th>\n      <th>23</th>\n      <th>24</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>tokens</th>\n      <td>&lt;s&gt;</td>\n      <td>▁2.000</td>\n      <td>▁Einwohner</td>\n      <td>n</td>\n      <td>▁an</td>\n      <td>▁der</td>\n      <td>▁Dan</td>\n      <td>zi</td>\n      <td>ger</td>\n      <td>▁Buch</td>\n      <td>t</td>\n      <td>▁in</td>\n      <td>▁der</td>\n      <td>▁polni</td>\n      <td>schen</td>\n      <td>▁Wo</td>\n      <td>i</td>\n      <td>wod</td>\n      <td>schaft</td>\n      <td>▁Po</td>\n      <td>mmer</td>\n      <td>n</td>\n      <td>▁</td>\n      <td>.</td>\n      <td>&lt;/s&gt;</td>\n    </tr>\n    <tr>\n      <th>labels</th>\n      <td>-100</td>\n      <td>0</td>\n      <td>0</td>\n      <td>-100</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>-100</td>\n      <td>-100</td>\n      <td>6</td>\n      <td>-100</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>-100</td>\n      <td>5</td>\n      <td>-100</td>\n      <td>-100</td>\n      <td>-100</td>\n      <td>6</td>\n      <td>-100</td>\n      <td>-100</td>\n      <td>0</td>\n      <td>-100</td>\n      <td>-100</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def encode_panx_dataset(corpus):\n    return corpus.map(tokenize_and_align_labels, batched=True, remove_columns=['langs', 'ner_tags', 'tokens'])","metadata":{"execution":{"iopub.status.busy":"2022-12-31T14:51:58.010182Z","iopub.execute_input":"2022-12-31T14:51:58.010938Z","iopub.status.idle":"2022-12-31T14:51:58.015777Z","shell.execute_reply.started":"2022-12-31T14:51:58.010896Z","shell.execute_reply":"2022-12-31T14:51:58.014806Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"panx_de_encoded = encode_panx_dataset(panx[\"de\"])","metadata":{"execution":{"iopub.status.busy":"2022-12-31T14:51:58.017162Z","iopub.execute_input":"2022-12-31T14:51:58.018029Z","iopub.status.idle":"2022-12-31T14:52:01.867518Z","shell.execute_reply.started":"2022-12-31T14:51:58.017989Z","shell.execute_reply":"2022-12-31T14:52:01.865830Z"},"trusted":true},"execution_count":26,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/13 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"408c08a3dc2744b6a1196d54a11e189a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/7 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aab3bca418af404db54ee31d84feb4c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/7 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e93516515a28426eb1c1be493e9031aa"}},"metadata":{}}]},{"cell_type":"code","source":"panx_de_encoded","metadata":{"execution":{"iopub.status.busy":"2022-12-31T14:52:01.870246Z","iopub.execute_input":"2022-12-31T14:52:01.870760Z","iopub.status.idle":"2022-12-31T14:52:01.879838Z","shell.execute_reply.started":"2022-12-31T14:52:01.870695Z","shell.execute_reply":"2022-12-31T14:52:01.878583Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 12580\n    })\n    validation: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 6290\n    })\n    test: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 6290\n    })\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"## Performance measures\n\nIn NER, *all* words of an entity must be predicted correctly in order for a prediction to be counted as correct. The library `seqeval` is designed for these kinds of tasks.","metadata":{}},{"cell_type":"code","source":"!pip install seqeval","metadata":{"execution":{"iopub.status.busy":"2022-12-31T14:52:01.886623Z","iopub.execute_input":"2022-12-31T14:52:01.887225Z","iopub.status.idle":"2022-12-31T14:52:25.679316Z","shell.execute_reply.started":"2022-12-31T14:52:01.887166Z","shell.execute_reply":"2022-12-31T14:52:25.676309Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nCollecting seqeval\n  Downloading seqeval-1.2.2.tar.gz (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m182.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.7/site-packages (from seqeval) (1.21.6)\nRequirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.7/site-packages (from seqeval) (1.0.2)\nRequirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval) (1.7.3)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval) (1.0.1)\nBuilding wheels for collected packages: seqeval\n  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16180 sha256=a6832da33be16cb6a116583355eea4410120f01fb5c53a0309376c4478f30ad2\n  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\nSuccessfully built seqeval\nInstalling collected packages: seqeval\nSuccessfully installed seqeval-1.2.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"def align_predictions(predictions, label_ids):\n    \"\"\"Prepare model outputs for metric evaluation by seqeval\"\"\"\n    preds = np.argmax(predictions, axis=2)\n    batch_size, seq_len = preds.shape\n    labels_list, preds_list = [], []\n    \n    for batch_idx in range(batch_size):\n        example_labels, example_preds = [], []\n        for seq_idx in range(seq_len):\n            # Ignore label IDs ignore_index_value_in_loss\n            if label_ids[batch_idx, seq_idx] != ignore_index_value_in_loss:\n                example_labels.append(index2tag[label_ids[batch_idx][seq_idx]])\n                example_preds.append(index2tag[preds[batch_idx][seq_idx]])\n        \n        labels_list.append(example_labels)\n        preds_list.append(example_preds)\n    return preds_list, labels_list","metadata":{"execution":{"iopub.status.busy":"2022-12-31T14:52:25.682577Z","iopub.execute_input":"2022-12-31T14:52:25.683141Z","iopub.status.idle":"2022-12-31T14:52:25.697224Z","shell.execute_reply.started":"2022-12-31T14:52:25.683086Z","shell.execute_reply":"2022-12-31T14:52:25.695893Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"## Fine-tuning XLM-Roberta\n\nStrategy:\n- Fine-tune on the German (largest) subset of PAN-X\n- Evaluate zero-shot cross-lingual performance on French, Italian, and English","metadata":{}},{"cell_type":"code","source":"from transformers import TrainingArguments","metadata":{"execution":{"iopub.status.busy":"2022-12-31T14:52:25.698867Z","iopub.execute_input":"2022-12-31T14:52:25.699480Z","iopub.status.idle":"2022-12-31T14:52:32.258582Z","shell.execute_reply.started":"2022-12-31T14:52:25.699426Z","shell.execute_reply":"2022-12-31T14:52:32.257455Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"num_epochs = 3\nbatch_size=24\nlogging_steps = len(panx_de_encoded[\"train\"]) // batch_size\nmodel_name = f'{xlmr_model_name}-finetuned-panx-de'\ntraining_args = TrainingArguments(\n    output_dir=f'/kaggle/working/{model_name}', log_level=\"error\", num_train_epochs=num_epochs, \n    per_device_eval_batch_size=batch_size, per_gpu_train_batch_size=batch_size, evaluation_strategy=\"epoch\",\n    save_steps=1e6, weight_decay=0.01, disable_tqdm=False, logging_steps=logging_steps, push_to_hub=False\n)","metadata":{"execution":{"iopub.status.busy":"2022-12-31T14:52:32.260513Z","iopub.execute_input":"2022-12-31T14:52:32.260998Z","iopub.status.idle":"2022-12-31T14:52:32.277398Z","shell.execute_reply.started":"2022-12-31T14:52:32.260956Z","shell.execute_reply":"2022-12-31T14:52:32.275916Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"from seqeval.metrics import f1_score\n\ndef compute_metrics(eval_pred):\n    y_pred, y_true = align_predictions(eval_pred.predictions, eval_pred.label_ids)\n    return {\"f1\": f1_score(y_true, y_pred)}","metadata":{"execution":{"iopub.status.busy":"2022-12-31T14:52:32.279641Z","iopub.execute_input":"2022-12-31T14:52:32.280598Z","iopub.status.idle":"2022-12-31T14:52:32.940164Z","shell.execute_reply.started":"2022-12-31T14:52:32.280548Z","shell.execute_reply":"2022-12-31T14:52:32.938595Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"Define a *data collator* to pad each input sequence to the largest sequence length in a batch.","metadata":{}},{"cell_type":"code","source":"from transformers import DataCollatorForTokenClassification","metadata":{"execution":{"iopub.status.busy":"2022-12-31T14:52:32.942016Z","iopub.execute_input":"2022-12-31T14:52:32.942471Z","iopub.status.idle":"2022-12-31T14:52:33.077848Z","shell.execute_reply.started":"2022-12-31T14:52:32.942433Z","shell.execute_reply":"2022-12-31T14:52:33.076313Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"data_collator = DataCollatorForTokenClassification(xlmr_tokenizer)","metadata":{"execution":{"iopub.status.busy":"2022-12-31T14:52:33.079428Z","iopub.execute_input":"2022-12-31T14:52:33.080426Z","iopub.status.idle":"2022-12-31T14:52:33.087634Z","shell.execute_reply.started":"2022-12-31T14:52:33.080373Z","shell.execute_reply":"2022-12-31T14:52:33.085759Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"Definte a `model_init` function so that we can avoid initializing a new model for every `Trainer` ","metadata":{}},{"cell_type":"code","source":"def model_init():\n    return (XLMRobertaForTokenClassification\n            .from_pretrained(xlmr_model_name, config=xlmr_config)\n            .to(device))","metadata":{"execution":{"iopub.status.busy":"2022-12-31T14:52:33.089657Z","iopub.execute_input":"2022-12-31T14:52:33.090243Z","iopub.status.idle":"2022-12-31T14:52:33.102897Z","shell.execute_reply.started":"2022-12-31T14:52:33.090115Z","shell.execute_reply":"2022-12-31T14:52:33.101390Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"Define trainer","metadata":{}},{"cell_type":"code","source":"from transformers import Trainer","metadata":{"execution":{"iopub.status.busy":"2022-12-31T14:52:33.105052Z","iopub.execute_input":"2022-12-31T14:52:33.105521Z","iopub.status.idle":"2022-12-31T14:52:33.382720Z","shell.execute_reply.started":"2022-12-31T14:52:33.105484Z","shell.execute_reply":"2022-12-31T14:52:33.381483Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"str(device)","metadata":{"execution":{"iopub.status.busy":"2022-12-31T14:52:33.384387Z","iopub.execute_input":"2022-12-31T14:52:33.385518Z","iopub.status.idle":"2022-12-31T14:52:33.394184Z","shell.execute_reply.started":"2022-12-31T14:52:33.385474Z","shell.execute_reply":"2022-12-31T14:52:33.392846Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"'cpu'"},"metadata":{}}]},{"cell_type":"code","source":"trained_path = f'/kaggle/working/{model_name}/trained'\n\nif str(device) != 'cpu':\n    trainer = Trainer(model_init=model_init, args=training_args, data_collator=data_collator, compute_metrics=compute_metrics,\n                      train_dataset=panx_de_encoded[\"train\"], \n                      eval_dataset=panx_de_encoded[\"validation\"],\n                      tokenizer=xlmr_tokenizer,\n                     )\n    trainer.train()\n    trainer.save_model(trained_path)\n    # Switch back to CPU when done","metadata":{"execution":{"iopub.status.busy":"2022-12-31T14:52:33.395812Z","iopub.execute_input":"2022-12-31T14:52:33.396230Z","iopub.status.idle":"2022-12-31T14:52:33.407444Z","shell.execute_reply.started":"2022-12-31T14:52:33.396191Z","shell.execute_reply":"2022-12-31T14:52:33.405797Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"model = XLMRobertaForTokenClassification.from_pretrained(trained_path).to(device)","metadata":{"execution":{"iopub.status.busy":"2022-12-31T14:52:33.410612Z","iopub.execute_input":"2022-12-31T14:52:33.411202Z","iopub.status.idle":"2022-12-31T14:52:39.586165Z","shell.execute_reply.started":"2022-12-31T14:52:33.411150Z","shell.execute_reply":"2022-12-31T14:52:39.584661Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"# test on an example text\ntext_de = \"Jeff Dean ist ein Informatiker bei Google in Kalifornien\"\nwith full_context:\n    display(tag_text(text_de, tags, model, xlmr_tokenizer))","metadata":{"execution":{"iopub.status.busy":"2022-12-31T14:53:09.272856Z","iopub.execute_input":"2022-12-31T14:53:09.273540Z","iopub.status.idle":"2022-12-31T14:53:09.417143Z","shell.execute_reply.started":"2022-12-31T14:53:09.273487Z","shell.execute_reply":"2022-12-31T14:53:09.415907Z"},"trusted":true},"execution_count":41,"outputs":[{"output_type":"display_data","data":{"text/plain":"         0      1      2      3     4     5           6    7     8        9   \\\nTokens  <s>  ▁Jeff    ▁De     an  ▁ist  ▁ein  ▁Informati  ker  ▁bei  ▁Google   \nTags      O  B-PER  I-PER  I-PER     O     O           O    O     O    B-ORG   \n\n         10          11     12    13  \nTokens  ▁in  ▁Kaliforni     en  </s>  \nTags      O       B-LOC  I-LOC     O  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n      <th>12</th>\n      <th>13</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Tokens</th>\n      <td>&lt;s&gt;</td>\n      <td>▁Jeff</td>\n      <td>▁De</td>\n      <td>an</td>\n      <td>▁ist</td>\n      <td>▁ein</td>\n      <td>▁Informati</td>\n      <td>ker</td>\n      <td>▁bei</td>\n      <td>▁Google</td>\n      <td>▁in</td>\n      <td>▁Kaliforni</td>\n      <td>en</td>\n      <td>&lt;/s&gt;</td>\n    </tr>\n    <tr>\n      <th>Tags</th>\n      <td>O</td>\n      <td>B-PER</td>\n      <td>I-PER</td>\n      <td>I-PER</td>\n      <td>O</td>\n      <td>O</td>\n      <td>O</td>\n      <td>O</td>\n      <td>O</td>\n      <td>B-ORG</td>\n      <td>O</td>\n      <td>B-LOC</td>\n      <td>I-LOC</td>\n      <td>O</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Error analysis","metadata":{}},{"cell_type":"markdown","source":"Get loss and predicted labels for validation set, and then observe examples with highest loss","metadata":{}},{"cell_type":"code","source":"from torch.nn.functional import cross_entropy","metadata":{"execution":{"iopub.status.busy":"2022-12-31T15:00:08.232416Z","iopub.execute_input":"2022-12-31T15:00:08.232923Z","iopub.status.idle":"2022-12-31T15:00:08.238810Z","shell.execute_reply.started":"2022-12-31T15:00:08.232881Z","shell.execute_reply":"2022-12-31T15:00:08.237487Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"def forward_pass_with_label(batch):\n    # prepare features for data collator by converting dict of lists to list of dicts\n    features = [dict(zip(batch, t)) for t in zip(*batch.values())]\n    batch = data_collator(features)\n    input_ids = batch[\"input_ids\"].to(device)\n    attention_mask = batch[\"attention_mask\"].to(device)\n    labels = batch[\"labels\"].to(device)\n    with torch.no_grad():\n        output = model(input_ids, attention_mask)\n        # logit.size = [batch_size, sequence_length, classes]\n        predicted_label = torch.argmax(output.logits, axis=-1).cpu().numpy()\n    loss = cross_entropy(output.logits.view(-1, 7), labels.view(-1), reduction=\"none\")\n    loss = loss.view(len(input_ids), -1).cpu().numpy()\n    return {'loss': loss, 'predicted_label': predicted_label}","metadata":{"execution":{"iopub.status.busy":"2022-12-31T15:18:09.649794Z","iopub.execute_input":"2022-12-31T15:18:09.650418Z","iopub.status.idle":"2022-12-31T15:18:09.658842Z","shell.execute_reply.started":"2022-12-31T15:18:09.650381Z","shell.execute_reply":"2022-12-31T15:18:09.657797Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"if str(device) != \"cpu\":\n    valid_set = panx_de_encoded[\"validation\"]\n    valid_set = valid_set.map(forward_pass_with_label, batched=True, batch_size=32)\n    valid_set.save_to_disk(f'/kaggle/working/{model_name}/validation/valid_set')","metadata":{"execution":{"iopub.status.busy":"2022-12-31T15:18:10.238817Z","iopub.execute_input":"2022-12-31T15:18:10.240130Z","iopub.status.idle":"2022-12-31T15:24:55.363996Z","shell.execute_reply.started":"2022-12-31T15:18:10.240065Z","shell.execute_reply":"2022-12-31T15:24:55.362615Z"},"trusted":true},"execution_count":46,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/197 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b73a7353209148e3a084f8446511dfa9"}},"metadata":{}}]},{"cell_type":"code","source":"from datasets import load_from_disk\n\nvalid_set = load_from_disk(f'/kaggle/working/{model_name}/validation/valid_set')","metadata":{"execution":{"iopub.status.busy":"2022-12-31T15:31:22.920684Z","iopub.execute_input":"2022-12-31T15:31:22.921586Z","iopub.status.idle":"2022-12-31T15:31:22.937061Z","shell.execute_reply.started":"2022-12-31T15:31:22.921538Z","shell.execute_reply":"2022-12-31T15:31:22.935741Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"df = valid_set.to_pandas()","metadata":{"execution":{"iopub.status.busy":"2022-12-31T15:31:25.031610Z","iopub.execute_input":"2022-12-31T15:31:25.032102Z","iopub.status.idle":"2022-12-31T15:31:25.058339Z","shell.execute_reply.started":"2022-12-31T15:31:25.032067Z","shell.execute_reply":"2022-12-31T15:31:25.056940Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-31T15:31:55.212142Z","iopub.execute_input":"2022-12-31T15:31:55.215036Z","iopub.status.idle":"2022-12-31T15:31:55.247407Z","shell.execute_reply.started":"2022-12-31T15:31:55.214990Z","shell.execute_reply":"2022-12-31T15:31:55.245909Z"},"trusted":true},"execution_count":54,"outputs":[{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"                                           input_ids  \\\n0                 [0, 10699, 11, 15, 16104, 1388, 2]   \n1  [0, 56530, 25216, 30121, 152385, 19229, 83982,...   \n2  [0, 159093, 165, 38506, 122, 153080, 29088, 57...   \n3     [0, 16459, 242, 5106, 6, 198715, 5106, 242, 2]   \n4  [0, 11022, 2315, 7418, 1079, 8186, 57242, 97, ...   \n\n                                      attention_mask  \\\n0                              [1, 1, 1, 1, 1, 1, 1]   \n1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n2         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n3                        [1, 1, 1, 1, 1, 1, 1, 1, 1]   \n4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n\n                                              labels  \\\n0                     [-100, 3, -100, 4, 4, 4, -100]   \n1  [-100, 0, -100, -100, -100, -100, 3, -100, -10...   \n2  [-100, 0, 0, 0, 0, 3, -100, -100, 0, -100, 0, ...   \n3               [-100, 0, 0, 0, 5, -100, 0, 0, -100]   \n4  [-100, 0, 0, 0, 0, 0, 0, 0, -100, 0, 0, 0, 3, ...   \n\n                                                loss  \\\n0  [0.0, 0.010982071, 0.0, 0.015253793, 0.0107771...   \n1  [0.0, 0.00022337325, 0.0, 0.0, 0.0, 0.0, 0.561...   \n2  [0.0, 0.00027533554, 0.00012671144, 0.00023159...   \n3  [0.0, 0.00037817957, 0.00020287363, 0.00020859...   \n4  [0.0, 0.00014780859, 0.00013469743, 0.00013815...   \n\n                                     predicted_label  \n0  [4, 3, 4, 4, 4, 4, 0, 3, 3, 3, 3, 3, 3, 3, 3, ...  \n1  [0, 0, 0, 0, 0, 0, 3, 4, 4, 4, 4, 4, 4, 4, 4, ...  \n2  [0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n3  [0, 0, 0, 0, 5, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>input_ids</th>\n      <th>attention_mask</th>\n      <th>labels</th>\n      <th>loss</th>\n      <th>predicted_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[0, 10699, 11, 15, 16104, 1388, 2]</td>\n      <td>[1, 1, 1, 1, 1, 1, 1]</td>\n      <td>[-100, 3, -100, 4, 4, 4, -100]</td>\n      <td>[0.0, 0.010982071, 0.0, 0.015253793, 0.0107771...</td>\n      <td>[4, 3, 4, 4, 4, 4, 0, 3, 3, 3, 3, 3, 3, 3, 3, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[0, 56530, 25216, 30121, 152385, 19229, 83982,...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n      <td>[-100, 0, -100, -100, -100, -100, 3, -100, -10...</td>\n      <td>[0.0, 0.00022337325, 0.0, 0.0, 0.0, 0.0, 0.561...</td>\n      <td>[0, 0, 0, 0, 0, 0, 3, 4, 4, 4, 4, 4, 4, 4, 4, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[0, 159093, 165, 38506, 122, 153080, 29088, 57...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n      <td>[-100, 0, 0, 0, 0, 3, -100, -100, 0, -100, 0, ...</td>\n      <td>[0.0, 0.00027533554, 0.00012671144, 0.00023159...</td>\n      <td>[0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[0, 16459, 242, 5106, 6, 198715, 5106, 242, 2]</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n      <td>[-100, 0, 0, 0, 5, -100, 0, 0, -100]</td>\n      <td>[0.0, 0.00037817957, 0.00020287363, 0.00020859...</td>\n      <td>[0, 0, 0, 0, 5, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[0, 11022, 2315, 7418, 1079, 8186, 57242, 97, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n      <td>[-100, 0, 0, 0, 0, 0, 0, 0, -100, 0, 0, 0, 3, ...</td>\n      <td>[0.0, 0.00014780859, 0.00013469743, 0.00013815...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"index2tag[-100] = \"IGN\"  # ignore code\ndf[\"input_tokens\"] = df[\"input_ids\"].apply(lambda x: xlmr_tokenizer.convert_ids_to_tokens(x))\ndf[\"predicted_label\"] = df[\"predicted_label\"].apply(lambda x: [index2tag[i] for i in x])","metadata":{"execution":{"iopub.status.busy":"2022-12-31T15:33:15.051609Z","iopub.execute_input":"2022-12-31T15:33:15.052121Z","iopub.status.idle":"2022-12-31T15:33:15.356456Z","shell.execute_reply.started":"2022-12-31T15:33:15.052083Z","shell.execute_reply":"2022-12-31T15:33:15.355144Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"df[\"labels\"] = df['labels'].apply(lambda x: [index2tag[i] for i in x])\ndf[\"loss\"] = df.apply(lambda x: x['loss'][:len(x['input_ids'])], axis=1)\ndf[\"predicted_label\"] = df.apply(lambda x: x['predicted_label'][:len(x['input_ids'])], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-12-31T15:35:08.747055Z","iopub.execute_input":"2022-12-31T15:35:08.747633Z","iopub.status.idle":"2022-12-31T15:35:09.013111Z","shell.execute_reply.started":"2022-12-31T15:35:08.747589Z","shell.execute_reply":"2022-12-31T15:35:09.011753Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-31T15:35:14.477254Z","iopub.execute_input":"2022-12-31T15:35:14.477743Z","iopub.status.idle":"2022-12-31T15:35:14.515862Z","shell.execute_reply.started":"2022-12-31T15:35:14.477693Z","shell.execute_reply":"2022-12-31T15:35:14.514773Z"},"trusted":true},"execution_count":57,"outputs":[{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"                                           input_ids  \\\n0                 [0, 10699, 11, 15, 16104, 1388, 2]   \n1  [0, 56530, 25216, 30121, 152385, 19229, 83982,...   \n2  [0, 159093, 165, 38506, 122, 153080, 29088, 57...   \n3     [0, 16459, 242, 5106, 6, 198715, 5106, 242, 2]   \n4  [0, 11022, 2315, 7418, 1079, 8186, 57242, 97, ...   \n\n                                      attention_mask  \\\n0                              [1, 1, 1, 1, 1, 1, 1]   \n1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n2         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n3                        [1, 1, 1, 1, 1, 1, 1, 1, 1]   \n4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n\n                                              labels  \\\n0        [IGN, B-ORG, IGN, I-ORG, I-ORG, I-ORG, IGN]   \n1  [IGN, O, IGN, IGN, IGN, IGN, B-ORG, IGN, IGN, ...   \n2  [IGN, O, O, O, O, B-ORG, IGN, IGN, O, IGN, O, ...   \n3              [IGN, O, O, O, B-LOC, IGN, O, O, IGN]   \n4  [IGN, O, O, O, O, O, O, O, IGN, O, O, O, B-ORG...   \n\n                                                loss  \\\n0  [0.0, 0.010982071, 0.0, 0.015253793, 0.0107771...   \n1  [0.0, 0.00022337325, 0.0, 0.0, 0.0, 0.0, 0.561...   \n2  [0.0, 0.00027533554, 0.00012671144, 0.00023159...   \n3  [0.0, 0.00037817957, 0.00020287363, 0.00020859...   \n4  [0.0, 0.00014780859, 0.00013469743, 0.00013815...   \n\n                                     predicted_label  \\\n0      [I-ORG, B-ORG, I-ORG, I-ORG, I-ORG, I-ORG, O]   \n1  [O, O, O, O, O, O, B-ORG, I-ORG, I-ORG, I-ORG,...   \n2     [O, O, O, O, O, B-ORG, O, O, O, O, O, O, O, O]   \n3                [O, O, O, O, B-LOC, I-LOC, O, O, O]   \n4  [O, O, O, O, O, O, O, O, O, O, O, O, B-ORG, I-...   \n\n                                        input_tokens  \n0         [<s>, ▁Ham, a, ▁(, ▁Unternehmen, ▁), </s>]  \n1  [<s>, ▁WE, ITE, RL, EIT, UNG, ▁Luz, ky, j, ▁a,...  \n2  [<s>, ▁entdeckt, ▁und, ▁gehört, ▁der, ▁Spek, t...  \n3    [<s>, ▁**, ▁', ▁'', ▁, Bretagne, ▁'', ▁', </s>]  \n4  [<s>, ▁Nach, ▁einem, ▁Jahr, ▁bei, ▁diesem, ▁Ve...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>input_ids</th>\n      <th>attention_mask</th>\n      <th>labels</th>\n      <th>loss</th>\n      <th>predicted_label</th>\n      <th>input_tokens</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[0, 10699, 11, 15, 16104, 1388, 2]</td>\n      <td>[1, 1, 1, 1, 1, 1, 1]</td>\n      <td>[IGN, B-ORG, IGN, I-ORG, I-ORG, I-ORG, IGN]</td>\n      <td>[0.0, 0.010982071, 0.0, 0.015253793, 0.0107771...</td>\n      <td>[I-ORG, B-ORG, I-ORG, I-ORG, I-ORG, I-ORG, O]</td>\n      <td>[&lt;s&gt;, ▁Ham, a, ▁(, ▁Unternehmen, ▁), &lt;/s&gt;]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[0, 56530, 25216, 30121, 152385, 19229, 83982,...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n      <td>[IGN, O, IGN, IGN, IGN, IGN, B-ORG, IGN, IGN, ...</td>\n      <td>[0.0, 0.00022337325, 0.0, 0.0, 0.0, 0.0, 0.561...</td>\n      <td>[O, O, O, O, O, O, B-ORG, I-ORG, I-ORG, I-ORG,...</td>\n      <td>[&lt;s&gt;, ▁WE, ITE, RL, EIT, UNG, ▁Luz, ky, j, ▁a,...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[0, 159093, 165, 38506, 122, 153080, 29088, 57...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n      <td>[IGN, O, O, O, O, B-ORG, IGN, IGN, O, IGN, O, ...</td>\n      <td>[0.0, 0.00027533554, 0.00012671144, 0.00023159...</td>\n      <td>[O, O, O, O, O, B-ORG, O, O, O, O, O, O, O, O]</td>\n      <td>[&lt;s&gt;, ▁entdeckt, ▁und, ▁gehört, ▁der, ▁Spek, t...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[0, 16459, 242, 5106, 6, 198715, 5106, 242, 2]</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n      <td>[IGN, O, O, O, B-LOC, IGN, O, O, IGN]</td>\n      <td>[0.0, 0.00037817957, 0.00020287363, 0.00020859...</td>\n      <td>[O, O, O, O, B-LOC, I-LOC, O, O, O]</td>\n      <td>[&lt;s&gt;, ▁**, ▁', ▁'', ▁, Bretagne, ▁'', ▁', &lt;/s&gt;]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[0, 11022, 2315, 7418, 1079, 8186, 57242, 97, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n      <td>[IGN, O, O, O, O, O, O, O, IGN, O, O, O, B-ORG...</td>\n      <td>[0.0, 0.00014780859, 0.00013469743, 0.00013815...</td>\n      <td>[O, O, O, O, O, O, O, O, O, O, O, O, B-ORG, I-...</td>\n      <td>[&lt;s&gt;, ▁Nach, ▁einem, ▁Jahr, ▁bei, ▁diesem, ▁Ve...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df_tokens = df.apply(pd.Series.explode)\ndf_tokens = df_tokens.query(\"labels != 'IGN'\")\ndf_tokens.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-31T15:36:59.440914Z","iopub.execute_input":"2022-12-31T15:36:59.441341Z","iopub.status.idle":"2022-12-31T15:36:59.563609Z","shell.execute_reply.started":"2022-12-31T15:36:59.441304Z","shell.execute_reply":"2022-12-31T15:36:59.562525Z"},"trusted":true},"execution_count":58,"outputs":[{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"  input_ids attention_mask labels      loss predicted_label  input_tokens\n0     10699              1  B-ORG  0.010982           B-ORG          ▁Ham\n0        15              1  I-ORG  0.015254           I-ORG            ▁(\n0     16104              1  I-ORG  0.010777           I-ORG  ▁Unternehmen\n0      1388              1  I-ORG  0.018821           I-ORG            ▁)\n1     56530              1      O  0.000223               O           ▁WE","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>input_ids</th>\n      <th>attention_mask</th>\n      <th>labels</th>\n      <th>loss</th>\n      <th>predicted_label</th>\n      <th>input_tokens</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10699</td>\n      <td>1</td>\n      <td>B-ORG</td>\n      <td>0.010982</td>\n      <td>B-ORG</td>\n      <td>▁Ham</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>15</td>\n      <td>1</td>\n      <td>I-ORG</td>\n      <td>0.015254</td>\n      <td>I-ORG</td>\n      <td>▁(</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>16104</td>\n      <td>1</td>\n      <td>I-ORG</td>\n      <td>0.010777</td>\n      <td>I-ORG</td>\n      <td>▁Unternehmen</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>1388</td>\n      <td>1</td>\n      <td>I-ORG</td>\n      <td>0.018821</td>\n      <td>I-ORG</td>\n      <td>▁)</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>56530</td>\n      <td>1</td>\n      <td>O</td>\n      <td>0.000223</td>\n      <td>O</td>\n      <td>▁WE</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"(df_tokens.groupby(\"input_tokens\")[[\"loss\"]]\n .agg([\"count\", \"mean\", \"sum\"])\n .droplevel(level=0, axis=1)\n .sort_values(by=\"sum\", ascending=False)\n .reset_index()\n .round(2)\n .head(10)\n .T\n)","metadata":{"execution":{"iopub.status.busy":"2022-12-31T15:41:28.090195Z","iopub.execute_input":"2022-12-31T15:41:28.090654Z","iopub.status.idle":"2022-12-31T15:41:29.177122Z","shell.execute_reply.started":"2022-12-31T15:41:28.090620Z","shell.execute_reply":"2022-12-31T15:41:29.175731Z"},"trusted":true},"execution_count":63,"outputs":[{"execution_count":63,"output_type":"execute_result","data":{"text/plain":"                       0           1           2          3          4  \\\ninput_tokens           ▁        ▁der         ▁in       ▁von       ▁und   \ncount               6066        1388         989        808       1171   \nmean                0.04         0.1        0.13       0.14       0.08   \nsum           220.886353  136.082352  128.466568  116.45929  97.961334   \n\n                      5          6          7          8          9  \ninput_tokens         ▁(         ▁/         ▁)        ▁''       ▁die  \ncount               246        163        246       2898        860  \nmean               0.33       0.49        0.3       0.03       0.06  \nsum           80.860657  79.478256  72.718269  72.577118  49.512142  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>input_tokens</th>\n      <td>▁</td>\n      <td>▁der</td>\n      <td>▁in</td>\n      <td>▁von</td>\n      <td>▁und</td>\n      <td>▁(</td>\n      <td>▁/</td>\n      <td>▁)</td>\n      <td>▁''</td>\n      <td>▁die</td>\n    </tr>\n    <tr>\n      <th>count</th>\n      <td>6066</td>\n      <td>1388</td>\n      <td>989</td>\n      <td>808</td>\n      <td>1171</td>\n      <td>246</td>\n      <td>163</td>\n      <td>246</td>\n      <td>2898</td>\n      <td>860</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.04</td>\n      <td>0.1</td>\n      <td>0.13</td>\n      <td>0.14</td>\n      <td>0.08</td>\n      <td>0.33</td>\n      <td>0.49</td>\n      <td>0.3</td>\n      <td>0.03</td>\n      <td>0.06</td>\n    </tr>\n    <tr>\n      <th>sum</th>\n      <td>220.886353</td>\n      <td>136.082352</td>\n      <td>128.466568</td>\n      <td>116.45929</td>\n      <td>97.961334</td>\n      <td>80.860657</td>\n      <td>79.478256</td>\n      <td>72.718269</td>\n      <td>72.577118</td>\n      <td>49.512142</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"- Whitespace has largest total loss, but the mean loss is quite small\n- Words like \"in\", \"der\", \"von\" appear frequently and appear with named entities, which explains why they might get frequently mixed up\n- Parentheses and slashes seem to have quite a high loss. We could take a look at some corresponding example sequences and dig further into potential data issues.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2022-12-31T15:50:18.410657Z","iopub.execute_input":"2022-12-31T15:50:18.413670Z","iopub.status.idle":"2022-12-31T15:50:18.423556Z","shell.execute_reply.started":"2022-12-31T15:50:18.413601Z","shell.execute_reply":"2022-12-31T15:50:18.421642Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"def plot_confusion_matrix(y_true, y_preds, labels):\n    cm = confusion_matrix(y_true, y_preds, labels=labels, normalize='true')\n    fig, ax = plt.subplots(figsize=(6,6))\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n    disp.plot(cmap=\"Blues\", values_format=\".2f\", ax=ax, colorbar=False)\n    plt.title(\"Normalized confusion matrix\")\n","metadata":{"execution":{"iopub.status.busy":"2022-12-31T15:53:30.813697Z","iopub.execute_input":"2022-12-31T15:53:30.814189Z","iopub.status.idle":"2022-12-31T15:53:30.822939Z","shell.execute_reply.started":"2022-12-31T15:53:30.814153Z","shell.execute_reply":"2022-12-31T15:53:30.821197Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"df_tokens.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-31T15:51:47.114123Z","iopub.execute_input":"2022-12-31T15:51:47.115264Z","iopub.status.idle":"2022-12-31T15:51:47.129697Z","shell.execute_reply.started":"2022-12-31T15:51:47.115223Z","shell.execute_reply":"2022-12-31T15:51:47.128376Z"},"trusted":true},"execution_count":81,"outputs":[{"execution_count":81,"output_type":"execute_result","data":{"text/plain":"  input_ids attention_mask labels      loss predicted_label  input_tokens\n0     10699              1  B-ORG  0.010982           B-ORG          ▁Ham\n0        15              1  I-ORG  0.015254           I-ORG            ▁(\n0     16104              1  I-ORG  0.010777           I-ORG  ▁Unternehmen\n0      1388              1  I-ORG  0.018821           I-ORG            ▁)\n1     56530              1      O  0.000223               O           ▁WE","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>input_ids</th>\n      <th>attention_mask</th>\n      <th>labels</th>\n      <th>loss</th>\n      <th>predicted_label</th>\n      <th>input_tokens</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10699</td>\n      <td>1</td>\n      <td>B-ORG</td>\n      <td>0.010982</td>\n      <td>B-ORG</td>\n      <td>▁Ham</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>15</td>\n      <td>1</td>\n      <td>I-ORG</td>\n      <td>0.015254</td>\n      <td>I-ORG</td>\n      <td>▁(</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>16104</td>\n      <td>1</td>\n      <td>I-ORG</td>\n      <td>0.010777</td>\n      <td>I-ORG</td>\n      <td>▁Unternehmen</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>1388</td>\n      <td>1</td>\n      <td>I-ORG</td>\n      <td>0.018821</td>\n      <td>I-ORG</td>\n      <td>▁)</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>56530</td>\n      <td>1</td>\n      <td>O</td>\n      <td>0.000223</td>\n      <td>O</td>\n      <td>▁WE</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"plot_confusion_matrix(df_tokens[\"labels\"], df_tokens[\"predicted_label\"], tags.names)","metadata":{"execution":{"iopub.status.busy":"2022-12-31T15:53:33.470251Z","iopub.execute_input":"2022-12-31T15:53:33.470807Z","iopub.status.idle":"2022-12-31T15:53:34.047048Z","shell.execute_reply.started":"2022-12-31T15:53:33.470760Z","shell.execute_reply":"2022-12-31T15:53:34.045554Z"},"trusted":true},"execution_count":86,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x432 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":"- We can also look at sequences with a large total loss (for another time). This can help us discover mislabelled data.","metadata":{}},{"cell_type":"markdown","source":"## Cross-lingual transfer","metadata":{}},{"cell_type":"code","source":"trainer = Trainer(model, data_collator=data_collator, compute_metrics=compute_metrics)","metadata":{"execution":{"iopub.status.busy":"2022-12-31T16:31:41.885886Z","iopub.execute_input":"2022-12-31T16:31:41.886355Z","iopub.status.idle":"2022-12-31T16:31:41.906780Z","shell.execute_reply.started":"2022-12-31T16:31:41.886319Z","shell.execute_reply":"2022-12-31T16:31:41.905437Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"code","source":"def get_f1_score(trainer, dataset):\n    return trainer.predict(dataset).metrics['test_f1']","metadata":{"execution":{"iopub.status.busy":"2022-12-31T16:32:53.989210Z","iopub.execute_input":"2022-12-31T16:32:53.990932Z","iopub.status.idle":"2022-12-31T16:32:53.997017Z","shell.execute_reply.started":"2022-12-31T16:32:53.990873Z","shell.execute_reply":"2022-12-31T16:32:53.995675Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"code","source":"f1_scores = defaultdict(dict)\nf1_scores[\"de\"][\"de\"] = get_f1_score(trainer, panx_de_encoded[\"test\"])\nprint(f\"F1-score of [de] model on [de] dataset: {f1_scores['de']['de']:.3f}\")","metadata":{"execution":{"iopub.status.busy":"2022-12-31T16:32:56.709234Z","iopub.execute_input":"2022-12-31T16:32:56.709743Z","iopub.status.idle":"2022-12-31T16:38:37.330610Z","shell.execute_reply.started":"2022-12-31T16:32:56.709692Z","shell.execute_reply":"2022-12-31T16:38:37.329431Z"},"trusted":true},"execution_count":95,"outputs":[{"name":"stderr","text":"***** Running Prediction *****\n  Num examples = 6290\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1074' max='787' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [787/787 08:11]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"F1-score of [de] model on [de] dataset: 0.867\n","output_type":"stream"}]},{"cell_type":"code","source":"# Try a French example\ntext_fr = \"Jeff Dean est informaticien chez Google en Californie\"\nwith full_context:\n    display(tag_text(text_fr, tags, trainer.model, xlmr_tokenizer))","metadata":{"execution":{"iopub.status.busy":"2022-12-31T16:38:37.420376Z","iopub.execute_input":"2022-12-31T16:38:37.420770Z","iopub.status.idle":"2022-12-31T16:38:37.505056Z","shell.execute_reply.started":"2022-12-31T16:38:37.420737Z","shell.execute_reply":"2022-12-31T16:38:37.503762Z"},"trusted":true},"execution_count":97,"outputs":[{"output_type":"display_data","data":{"text/plain":"         0      1      2      3     4            5    6      7        8    9   \\\nTokens  <s>  ▁Jeff    ▁De     an  ▁est  ▁informatic  ien  ▁chez  ▁Google  ▁en   \nTags      O  B-PER  I-PER  I-PER     O            O    O      O    B-ORG    O   \n\n           10     11     12    13  \nTokens  ▁Cali    for    nie  </s>  \nTags    B-LOC  B-LOC  I-LOC     O  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n      <th>12</th>\n      <th>13</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Tokens</th>\n      <td>&lt;s&gt;</td>\n      <td>▁Jeff</td>\n      <td>▁De</td>\n      <td>an</td>\n      <td>▁est</td>\n      <td>▁informatic</td>\n      <td>ien</td>\n      <td>▁chez</td>\n      <td>▁Google</td>\n      <td>▁en</td>\n      <td>▁Cali</td>\n      <td>for</td>\n      <td>nie</td>\n      <td>&lt;/s&gt;</td>\n    </tr>\n    <tr>\n      <th>Tags</th>\n      <td>O</td>\n      <td>B-PER</td>\n      <td>I-PER</td>\n      <td>I-PER</td>\n      <td>O</td>\n      <td>O</td>\n      <td>O</td>\n      <td>O</td>\n      <td>B-ORG</td>\n      <td>O</td>\n      <td>B-LOC</td>\n      <td>B-LOC</td>\n      <td>I-LOC</td>\n      <td>O</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def evaluate_lang_performance(lang, trainer):\n    panx_ds = encode_panx_dataset(panx[lang])\n    return get_f1_score(trainer, panx_ds[\"test\"])","metadata":{"execution":{"iopub.status.busy":"2022-12-31T16:39:00.860999Z","iopub.execute_input":"2022-12-31T16:39:00.861606Z","iopub.status.idle":"2022-12-31T16:39:00.868697Z","shell.execute_reply.started":"2022-12-31T16:39:00.861544Z","shell.execute_reply":"2022-12-31T16:39:00.867422Z"},"trusted":true},"execution_count":99,"outputs":[]},{"cell_type":"code","source":"f1_scores[\"de\"][\"fr\"] = evaluate_lang_performance(\"fr\", trainer)\nprint(f\"F1-score of [de] model on [fr] dataset: {f1_scores['de']['fr']:.3f}\")","metadata":{"execution":{"iopub.status.busy":"2022-12-31T16:39:33.015150Z","iopub.execute_input":"2022-12-31T16:39:33.015682Z","iopub.status.idle":"2022-12-31T16:41:08.598219Z","shell.execute_reply.started":"2022-12-31T16:39:33.015637Z","shell.execute_reply":"2022-12-31T16:41:08.596814Z"},"trusted":true},"execution_count":100,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32a3c30c930a479ab2f5b8f4096a4fb5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"240c78a7779d468fb3db56437136e519"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e707736dc5a44ae7810fa13d93542727"}},"metadata":{}},{"name":"stderr","text":"***** Running Prediction *****\n  Num examples = 2290\n  Batch size = 8\n","output_type":"stream"},{"name":"stdout","text":"F1-score of [de] model on [fr] dataset: 0.703\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Impressive given that the model has not seen a single French label!","metadata":{}}]}